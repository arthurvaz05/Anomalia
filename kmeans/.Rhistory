time = rep(time, 4),
value = c(original, trend, seasonal, random),
variable = rep(c("original", "trend", "seasonal", "random"), each = length(time))
)
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Decompose the time series
decomposed_ts <- decompose(ts_data, type = "additive", period=1440)
# Convert Value column to a time series object
ts_data <- ts(data_pdt13a$Value, frequency = 1440)  # Assuming the data is sampled every minute
# Decompose the time series
decomposed_ts <- decompose(ts_data, type = "additive")
# Plot the decomposed components with title and xlabel
plot(decomposed_ts)
# Assuming your time series is stored in 'ts'
decomposed_ts <- decompose(ts)
# Decompose the time series
decomposed_ts <- decompose(ts_data, type = "additive")
# Plot the decomposed components with title and xlabel
plot(decomposed_ts)
# Extract the components from the decomposed object
time <- time(decomposed_ts$x)
original <- decomposed_ts$x
trend <- decomposed_ts$trend
seasonal <- decomposed_ts$seasonal
random <- decomposed_ts$random
# Combine the components into a single data frame
decomposed_melted <- data.frame(
time = rep(time, 4),
value = c(original, trend, seasonal, random),
variable = rep(c("original", "trend", "seasonal", "random"), each = length(time))
)
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Calculate the ACF
acf_value <- acf(ts_data, lag.max = 2000, plot = TRUE, main = glue("Autocorrelation Function Plot - {tag}")) # Adjust lag.max as needed
time
time <- data_pdt13a$TimeStamp
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
time
# Combine the components into a single data frame
decomposed_melted <- data.frame(
time = rep(time, 4),
value = c(original, trend, seasonal, random),
variable = rep(c("original", "trend", "seasonal", "random"), each = length(time))
)
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
decomposed_melted%>%head(3)
# Date for the vertical dashed line
vertical_line_date <- as.Date("2024-10-24")
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
geom_vline(xintercept = as.numeric(vertical_line_date), linetype = "dashed", color = "red") + # Add vertical dashed line
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
decomposed_melted%>%head(3)
# Date for the vertical dashed line
vertical_line_date <- as.Date("2024-10-24")
# Convert the date to numeric
vertical_line_numeric <- as.numeric(vertical_line_date)
# Ensure the date is within the range of the time series
if (vertical_line_numeric < min(time) | vertical_line_numeric > max(time)) {
stop("Vertical line date is outside the range of the time series data.")
}
vertical_line_numeric
min(time)
# Date for the vertical dashed line
vertical_line_date <- as.Date("2024-10-24")
# Convert the date to numeric
vertical_line_numeric <- as.numeric(difftime(vertical_line_date, as.Date("1970-01-01")))
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
geom_vline(xintercept = vertical_line_numeric, linetype = "dashed", color = "red") + # Add vertical dashed line
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
time
# Date for the vertical dashed line
vertical_line_date <- as.Date("2023-10-24")
# Convert the date to numeric
vertical_line_numeric <- as.numeric(difftime(vertical_line_date, as.Date("1970-01-01")))
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
geom_vline(xintercept = vertical_line_numeric, linetype = "dashed", color = "red") + # Add vertical dashed line
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Date for the vertical dashed line
vertical_line_date <- as.Date("2023-10-24")
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
geom_vline(xintercept = as.numeric(vertical_line_date), linetype = "dashed", color = "red") + # Add vertical dashed line
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# check the type of the time series
class(decomposed_melted)
# check the type of each column in the dataframe
str(decomposed_melted)
# Date for the vertical dashed line
vertical_line_date <- as.POSIXct("2024-10-24")
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
geom_vline(xintercept = as.numeric(vertical_line_date), linetype = "dashed", color = "red") + # Add vertical dashed line
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Calculate the ACF
acf_value <- acf(ts_data, lag.max = 2000, plot = TRUE, main = glue("Autocorrelation Function Plot - {tag}")) # Adjust lag.max as needed
# Combine the components into a single data frame
decomposed_melted <- data.frame(
time = rep(time, 4),
value = c(original, trend, seasonal, random),
variable = rep(c("original", "trend", "seasonal", "random"), each = length(time))
)
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = "Decomposed Time Series",
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
data_pdt13a <- data[data$TagName=="CNCO_GAS.analog.BM2gPDT13A.curval",]
tag <- data_pdt13a %>%
distinct(TagName) %>% str_split("\\.")
tag <- tag[[1]][3]%>%str_split("g")
tag <- tag[[1]][2]
# Convert Value column to a time series object
ts_data <- ts(data_pdt13a$Value, frequency = 1440)  # Assuming the data is sampled every minute
# Decompose the time series
decomposed_ts <- decompose(ts_data, type = "additive")
# Plot the decomposed components with title and xlabel
plot(decomposed_ts)
# Extract the components from the decomposed object
time <- data_pdt13a$TimeStamp
original <- decomposed_ts$x
trend <- decomposed_ts$trend
seasonal <- decomposed_ts$seasonal
random <- decomposed_ts$random
# Combine the components into a single data frame
decomposed_melted <- data.frame(
time = rep(time, 4),
value = c(original, trend, seasonal, random),
variable = rep(c("original", "trend", "seasonal", "random"), each = length(time))
)
# Plot using ggplot2 with facets
ggplot(decomposed_melted, aes(x = time, y = value, color = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
# Calculate the ACF
acf_value <- acf(ts_data, lag.max = 2000, plot = TRUE, main = glue("Autocorrelation Function Plot - {tag}")) # Adjust lag.max as needed
ggplot(decomposed_melted, aes(x = time, y = value, color = variable, group = variable)) +
geom_line() +
labs(title = glue("Decomposed Time Series - {tag}"),
x = "Time",
y = "Value") +
facet_wrap(~variable, scales = "free_y", ncol = 1) +
theme_minimal()
source('/Users/arthurvaz/Desktop/CEFETRJ - Mestrado/Anomalia/Anomalia/carregar_bases.R')
###LOAD AND INSTALL
{
# Library to load and install if ins't already installed
#install.packages('pacman')
library(pacman)
vetor_pacotes <- c("ggplot2", "plotly", "dplyr", "kableExtra", "gridExtra",
"TSPred", "dbscan", "fpc", "factoextra", "fitdistrplus",
"logspline", "caret","RColorBrewer","R.filesets")
pacman::p_load(char = vetor_pacotes)
library(daltoolbox)
library(harbinger)
setwd('/Users/arthurvaz/Desktop/CEFETRJ - Mestrado/Anomalia/Anomalia/kmeans')
source('kmeans_anomalia_windows.R')
}
# List of dataset filenames (without the base directory)
filenames <- c(
"gecco_sample.RData",
"nab_sample.RData",
"oil_3w_sample.RData",
"rare_sample.RData",
"ucr_sample.RData",
"yahoo_sample.RData"
)
load(file.path(base_dir,filenames[1]))
rm(df_final)
rm(df_final)
View(gecco_sample)
sw = 10
serie = 'ph'
data <- gecco_sample
data10 <- ts_data(data[,serie], sw)
data_sw <- ts_data(data[,serie], sw)
rm(data10)
library(dbscan)
library(dbscan)
datamatrix <- matrix(data_sw,nrow = 104,ncol = 10)
datamatrix
data_sw <- ts_data(data[,serie], sw)
datamatrix <- matrix(data_sw)
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.53, minPts=3)
dbscanResult
hullplot(datamatrix, dbscanResult, main="DBSCAN")
library("fpc")
db <- fpc::dbscan(datamatrix, eps= 0.53, MinPts =3)
print(db)
plot(db, datamatrix, main = "DBSCAN", frame = FALSE)
vetor_pacotes <- c("ggplot2", "plotly", "dplyr", "kableExtra", "gridExtra",
"TSPred", "dbscan", "fpc", "factoextra", "fitdistrplus",
"logspline", "caret","RColorBrewer","R.filesets","dbscan","fpc")
pacman::p_load(char = vetor_pacotes)
library(daltoolbox)
library(harbinger)
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.53, minPts=3)
datamatrix_df = data.frame(datamatrix)
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.15, minPts=3)
datamatrix_df = data.frame(datamatrix)
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
dbscan::kNNdistplot(data_sw, k =  5)
abline(h = 0.15, lty = 2)
abline(h = 0.05, lty = 2)
abline(h = 0.025, lty = 2)
dbscan::kNNdistplot(data_sw, k =  5)
abline(h = 0.025, lty = 2)
datamatrix <- matrix(data_sw)
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=3)
datamatrix_df = data.frame(datamatrix)
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=5)
datamatrix_df = data.frame(datamatrix)
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
#dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=5)
dbscanResult <- fpc::dbscan(datamatrix, eps= 0.025, MinPts =5)
datamatrix_df = data.frame(datamatrix)
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
dbscanResult$cluster
dbscanResult$cluster%>%table()
dbscan::kNNdistplot(data_sw, k =  3)
abline(h = 0.025, lty = 2)
#dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=5)
dbscanResult <- fpc::dbscan(datamatrix, eps= 0.025, MinPts =3)
datamatrix_df = data.frame(datamatrix)
dbscanResult$cluster%>%table()
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
datamatrix <- matrix(data_sw)
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=5)
datamatrix_df = data.frame(datamatrix)
dbscanResult$cluster%>%table()
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
datamatrix <- matrix(data_sw)
datamatrix <- matrix(data_sw,ncol=1sw)
datamatrix <- matrix(data_sw,ncol=sw)
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=5)
datamatrix_df = data.frame(datamatrix)
dbscanResult$cluster%>%table()
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
datamatrix_df_anomalias_t = data.frame(t(datamatrix_df_anomalias))
datamatrix_df_anomalias_t = data.frame(t(datamatrix_df_anomalias))
datamatrix_df_anomalias_t
datamatrix_df_anomalias_t = datamatrix_df_anomalias_t[-"clusters",]
datamatrix_df_anomalias_t = subset(datamatrix_df_anomalias_t, rownames(datamatrix_df_anomalias_t) != "clusters")
datamatrix_df_anomalias_t
datamatrix_df_anomalias
dbscanResult
print(dbscanResult)
dbscanResult$cluster
datamatrix_df_anomalias
as.vector(data_sw)
datamatrix <- matrix(data_sw,ncol=sw)
dbscanResult <- dbscan::dbscan(datamatrix, eps=0.025, minPts=5)
datamatrix_df = data.frame(datamatrix)
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
datamatrix_df_anomalias
datamatrix_df_anomalias
dbscanResult$cluster%>%table()
# Load the file
load("kmeans_result.RData")
df_final%>%head()
datamatrix_df_anomalias.index()
datamatrix_df_anomalias.dtypes()
datamatrix_df_anomalias.str()
datamatrix_df_anomalias%>%index()
datamatrix_df_anomalias%>%str()
# Assuming datamatrix_df_anomalias is your dataframe
row_indices <- rownames(datamatrix_df_anomalias)
row_indices
# Assuming datamatrix_df_anomalias is your dataframe
idx <- rownames(datamatrix_df_anomalias)%>%as.numeric()
# 1 if the data index in idx otherwise 0
data$anomalia <- ifelse(1:nrow(data) %in% idx, 1, 0)
data$anomalia
data$anomalia%>%table()
data
# Create a contingency matrix
contingency_matrix <- table(data$event, data$anomalia)
# Rename the rows and columns of the contingency matrix for clarity
rownames(contingency_matrix) <- c("Not Anomaly", "Anomaly")
colnames(contingency_matrix) <- c("Not Event", "Event")
# Extracting values from the contingency matrix
true_positives <- contingency_matrix["Anomaly", "Event"]
false_positives <- contingency_matrix["Not Anomaly", "Event"]
false_negatives <- contingency_matrix["Anomaly", "Not Event"]
# Calculating recall
recall <- true_positives / (true_positives + false_negatives)
# Calculating precision
precision <- true_positives / (true_positives + false_positives)
# Calculating F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
# Print the results
cat("Recall:", recall, "\n")
cat("Precision:", precision, "\n")
cat("F1-score:", f1_score, "\n")
df_final%>%head()
# create a dataframe with a new column called idx and event equal TRUE
as.data.frame(idx)
result$event <- TRUE
# create a dataframe with a new column called idx and event equal TRUE
result <- as.data.frame(idx)
result$event <- TRUE
result
#reset index
rownames(result) <- NULL
df_final%>%head()
result$seq <- sw
result$variable <- serie
result$dataset <- 'gecco_sample'
# 1 if the data index in idx otherwise 0
data$anomalia <- ifelse(1:nrow(data) %in% idx, 1, 0)
# Create a contingency matrix
contingency_matrix <- table(data$event, data$anomalia)
# Rename the rows and columns of the contingency matrix for clarity
rownames(contingency_matrix) <- c("Not Anomaly", "Anomaly")
colnames(contingency_matrix) <- c("Not Event", "Event")
# Extracting values from the contingency matrix
true_positives <- contingency_matrix["Anomaly", "Event"]
false_positives <- contingency_matrix["Not Anomaly", "Event"]
false_negatives <- contingency_matrix["Anomaly", "Not Event"]
# Calculating recall
result$recall <- true_positives / (true_positives + false_negatives)
# Calculating precision
result$precision <- true_positives / (true_positives + false_positives)
# Calculating F1-score
result$f1_score <- 2 * (precision * recall) / (precision + recall)
result
df_final%>%head()
df_final$seq %>% as.table()
seq
df_final$seq
df_final$seq %>%table()
df_final$seqlen %>%table()
sw = 10
serie = 'ph'
data <- gecco_sample
data_sw <- ts_data(data[,serie], sw)
dbscan::kNNdistplot(data_sw, k =  3)
abline(h = 0.025, lty = 2)
eps = 0.025
minPts = 2
datamatrix <- matrix(data_sw,ncol=sw)
dbscanResult <- dbscan::dbscan(datamatrix, eps=eps, minPts=minPts)
datamatrix_df = data.frame(datamatrix)
dbscanResult$cluster%>%table()
datamatrix_df$clusters =  dbscanResult$cluster
datamatrix_df_anomalias = datamatrix_df %>% filter(clusters==0)
# Assuming datamatrix_df_anomalias is your dataframe
idx <- rownames(datamatrix_df_anomalias)%>%as.numeric()
# create a dataframe with a new column called idx and event equal TRUE
result <- as.data.frame(idx)
result$event <- TRUE
#reset index
rownames(result) <- NULL
result$seq <- sw
result$variable <- serie
result$dataset <- 'gecco_sample'
result$eps = eps
result$minPts = minPts
# 1 if the data index in idx otherwise 0
data$anomalia <- ifelse(1:nrow(data) %in% idx, 1, 0)
# Create a contingency matrix
contingency_matrix <- table(data$event, data$anomalia)
# Rename the rows and columns of the contingency matrix for clarity
rownames(contingency_matrix) <- c("Not Anomaly", "Anomaly")
colnames(contingency_matrix) <- c("Not Event", "Event")
# Extracting values from the contingency matrix
true_positives <- contingency_matrix["Anomaly", "Event"]
false_positives <- contingency_matrix["Not Anomaly", "Event"]
false_negatives <- contingency_matrix["Anomaly", "Not Event"]
# Calculating recall
result$recall <- true_positives / (true_positives + false_negatives)
# Calculating precision
result$precision <- true_positives / (true_positives + false_positives)
# Calculating F1-score
result$f1_score <- 2 * (precision * recall) / (precision + recall)
result
dbscan::kNNdistplot(data_sw, k =  3)
x = dbscan::kNNdistplot(data_sw, k =  3)
x
dbscan::kNNdistplot(data_sw, k =  3)
# Access the kNN distances
kNNdist <- kNNdist(data_sw, k = 3)
kNNdist
# Access the kNN distances
kNNdist <- kNNdist(data_sw, k = minPts)
dbscan::kNNdistplot(data_sw, k =  minPts)
kNNdist
# Sort the distances in ascending order
sorted_distances <- sort(kNNdistances)
# Sort the distances in ascending order
sorted_distances <- sort(kNNdist)
plot(sorted_distances, type = "l", xlab = "Point Index", ylab = "k-distance")
abline(h = 0, col = "gray", lty = 2)
sorted_distances
# Calculate the percentage change of the sorted distances
percentage_change <- c(0, diff(sorted_distances) / sorted_distances[-length(sorted_distances)] * 100)
# Plot the percentage change against the index of each point
plot(percentage_change, type = "l", xlab = "Point Index", ylab = "Percentage Change")
# Optionally, you can add grid lines for better visualization
abline(h = 0, col = "gray", lty = 2)
# Plot the percentage change against the index of each point
plot(percentage_change, type = "l", xlab = "Point Index", ylab = "Percentage Change")
# Find the index of the maximum percentage change
max_index <- which.max(percentage_change)
# Output the index
print(max_index)
sorted_distances[max_index]
